{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Self-Driving Car Project: Advanced Lane Finding\n",
    "**By Elisa Chang**\n",
    "___\n",
    "#### The goals of this project are:\n",
    "* To make the lane detection pipeline more robust to changes in lane color and changes in lighting conditions than when we first implemented the pipeline in the [CarND-LaneLines-P1](https://github.com/eeeja/CarND-LaneLines-P1) implementation.\n",
    "* To calculate the curvature of the lanes so that we can adjust the steering wheel correctly.\n",
    "___\n",
    "#### We do this by using the following techniques:\n",
    "Analysis:\n",
    "1. [Correct Radial Distortion](#correct-radial-distortion): Compute and correct the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "    * [Calibrate Camera](#calibrate-camera)\n",
    "    * [Undistort Image](#undistort-image)\n",
    "2. [Find Lane Lines](#find-lane-lines): Use color transforms & pixel gradients to detect lane line edges.\n",
    "    * [ID Best Color Maps](#id-best-color-maps)\n",
    "    * [Find Sobel Gradients](#find-sobel-gradients)\n",
    "3. [Calculate Lane Line Curvature](#calculate-lane-line-curvature): Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "    * [Calculate Birdseye Perspective](#calculate-birdseye-perspective)\n",
    "    * [Detect lane pixels](#detect-lane-pixels)\n",
    "    * [Determine Lane Curvature](#determine-lane-curvature)\n",
    "    \n",
    "Pipeline:\n",
    "4. Process test images\n",
    "5. Process test videos\n",
    "\n",
    "This notebook explores in detail the python implementation of the lane detection pipeline. For a more analytical overview of the pipeline, please see [Writeup](https://github.com/eeeja/CarND-LaneLines-P1/blob/master/P1.ipynb) located at './Writeup.md'\n",
    "___\n",
    "\n",
    "### <a name=\"radial-distortion\">Correct Radial Distortion</a>\n",
    "Image Distortion happens when a 3D object is transformed to a 2D image. There are two forms of image distortion: Radial and perspective distortion. Here we focus on correcting the radial distortion. Further down at [Calculate Birdseye Perspective](#calculate-birdseye-perspective) we will adjust for the perspective distortion. \n",
    "\n",
    "Radial distortion happens when curved lenses in cameras bend light rays too much at the edge of lenses-- edges of images appear more/less curved than they actually are.\n",
    "\n",
    "$$x_{corrected} = x(1+k_1r^2+k_2r^4+k_3r^6)$$\n",
    "$$y_{corrected} = y(1+k_1r^2+k_2r^4+k_3r^6)$$\n",
    "\n",
    "#### <a name=\"calibrate-camera\">Calibrate Camera</a>\n",
    "To fix radial distortion, first we begin by calibrating our camera. We do this by analyzing the checker images in the \"./camera_cal\" folder to determine what the camera matrix and distortion coefficients are. We need these values to understand how the camera lens distorts images when transforming from 3D objects to 2D images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Libraries used throughout pipeline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "# Install imagio to use moviepy\n",
    "import imageio\n",
    "imageio.plugins.ffmpeg.download()\n",
    "\n",
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use cv2 functions findChessboardCorners() and drawChessboardCorners() \n",
    "# to find and draw corners of an imageimport numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Number of corners inside the checker image in X and Y dimensions\n",
    "nx = 9# Manually counted corners within the image\n",
    "ny = 6# Manually counted corners within the image\n",
    "\n",
    "# Image 1: \n",
    "# Make a list of calibration images\n",
    "# Edited image in Paint so it could have a white background\n",
    "fname = './camera_cal/calibration6.jpg' \n",
    "img = cv2.imread(fname)\n",
    "\n",
    "# Image 2:\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Image 3:\n",
    "# Find the chessboard corners\n",
    "ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "checker_img = img.copy()\n",
    "# If found, draw corners\n",
    "if ret == True:\n",
    "    # Draw and display the corners\n",
    "    cv2.drawChessboardCorners(checker_img, (nx, ny), corners, ret)\n",
    "    plt.imshow(checker_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"undistort-image\">Undistort Image</a>\n",
    "We need to run the above for all the checker images so that we can have a collection of image (distorted) points and object (undistorted) points. \n",
    "\n",
    "    \"Image points\" refer to the x,y,z coordinates of an object as displayed on the image. \n",
    "    \"Object points\" refer to the x,y,z coordinates of an object as displayed in real life.\n",
    "    For both of these points, we assume the z coordinate is 0.\n",
    "\n",
    "Using the same code from above, below we process all images in the *camera_cal* folder to collect the image and object points. Images with the detected corners are saved as \"chessboardCorner_calibrationX.jpg\" images in the **output_images** folder. Once all corner coordinates are collected, we run through all the images again and undistort them with `cv2.calibrateCamera` and `cv2.undistort.` These images are saved as \"undistorted_calibrationX.jpg\" images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper method used in our final pipeline later on\n",
    "\n",
    "# Function that takes an image, object points, and image points\n",
    "# performs the camera calibration, image distortion correction and \n",
    "# returns the undistorted image\n",
    "def cal_undistort(img, objpoints, imgpoints):\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img.shape[0:2], None, None)\n",
    "    dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return dst\n",
    "\n",
    "# Helper function to get binary images\n",
    "# zeros_like returns same dtype, assume all dtype is np.uint8 so we could use 0-255 range\n",
    "def get_binary(img, thresh=(0,255)):\n",
    "    binary = np.uint8(np.zeros_like(img))\n",
    "    binary[(img>thresh[0]) & (img<=thresh[1])] = 255\n",
    "    return binary\n",
    "\n",
    "## All these methods assume the image has been converted to a gray-scale image\n",
    "# Function applies Sobel x or y, then takes an absolute value and applies a threshold.\n",
    "def abs_sobel_thresh(gray, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    if orient == 'x':\n",
    "        # Need to set as cv2.CV_64F to capture both positive and negative slope\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel))\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1,ksize=sobel_kernel))\n",
    "    # Rescale back to 8 bit integer\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    \n",
    "    # Create a copy and apply the threshold\n",
    "    binary_output = get_binary(scaled_sobel, thresh=thresh)\n",
    "    \n",
    "    return binary_output\n",
    "\n",
    "\n",
    "# Function applies Sobel x and y, then computes the magnitude of the gradient\n",
    "# and applies a threshold\n",
    "def mag_thresh(gray, sobel_kernel=3, thresh=(0, 255)):\n",
    "    # Take the gradient in x and y separately\n",
    "    sobelx = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0,ksize=sobel_kernel))\n",
    "    sobely = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1,ksize=sobel_kernel))\n",
    "    # Calculate the magnitude \n",
    "    abs_sobelxy = np.sqrt(0.1*sobely*sobely+sobelx*sobelx)\n",
    "    # Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255*abs_sobelxy/np.max(abs_sobelxy))\n",
    "    # Create a binary mask where mag thresholds are met\n",
    "    binary_output = get_binary(scaled_sobel, thresh=thresh)\n",
    "    \n",
    "    return binary_output\n",
    "\n",
    "# Define a function that applies Sobel x and y, then computes the direction of the gradient\n",
    "# and applies a threshold.\n",
    "def dir_threshold(gray, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    # Take the gradient in x and y separately\n",
    "    sobelx = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel))\n",
    "    sobely = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel))\n",
    "    # Take the absolute value of the x and y gradients\n",
    "    abs_sobelx = np.sqrt(sobelx*sobelx)\n",
    "    abs_sobely = np.sqrt(sobely*sobely)\n",
    "    # Use np.arctan2(abs_sobely, abs_sobelx) to calculate the direction of the gradient \n",
    "    grad_dir =  np.arctan2(abs_sobely, abs_sobelx)\n",
    "    \n",
    "    # Create a binary mask where direction thresholds are met\n",
    "    binary_output = get_binary(grad_dir,thresh=thresh)\n",
    "    \n",
    "    return binary_output\n",
    "\n",
    "# Helper function that will take an image and return the R & G and Value gradients\n",
    "def extract_lane_gradient(img,thresh=(0,255),dir_thresh=(0,1.5),ksize=13,RGB=0):\n",
    "    \"\"\"Input RGB Image\"\"\"\n",
    "    HSV_img = img.copy()\n",
    "    \n",
    "    \n",
    "    #Use only R & G values --> assume input image is RGB\n",
    "    HSV_img[:,:,2] = HSV_img[:,:,0]\n",
    "    \n",
    "    if RGB ==0:\n",
    "        HSV_img = cv2.cvtColor(HSV_img, cv2.COLOR_RGB2HSV)\n",
    "        # Image 2: Grayscale of only R&G Values\n",
    "        HSV_gray = HSV_img[:,:,2]\n",
    "    \n",
    "    else: \n",
    "        HSV_gray = cv2.cvtColor(HSV_img,cv2.COLOR_RGB2GRAY)\n",
    "        HSV_gray[HSV_gray<140]=140\n",
    "\n",
    "\n",
    "    # Image 3: x Gradient of only R&G Values\n",
    "    HSV_gradx = abs_sobel_thresh(HSV_gray, orient='x', sobel_kernel=ksize, thresh=thresh)\n",
    "    \n",
    "    # Image 4: y Gradient of only R&G Values\n",
    "    HSV_grady = abs_sobel_thresh(HSV_gray, orient='y', sobel_kernel=ksize, thresh=thresh)\n",
    "    \n",
    "    # Image 5: X & Y Gradient magnitude of only R&G Values\n",
    "    HSV_mag_binary = mag_thresh(HSV_gray, sobel_kernel=ksize, thresh=thresh)\n",
    "    \n",
    "    # Image 6: Only gradients that are  almost verticle-- most likely to be lane lines\n",
    "    HSV_dir_binary = dir_threshold(HSV_gray, sobel_kernel=ksize, thresh=dir_thresh)\n",
    "    \n",
    "    # Image 7: All gradients combined of only R&G Values \n",
    "    # Combine all gradients\n",
    "    HSV_combined = np.zeros_like(HSV_dir_binary)\n",
    "    HSV_combined[((HSV_gradx == 255) & (HSV_grady == 255)) \n",
    "                | ((HSV_mag_binary == 255) & (HSV_dir_binary == 255))] = 255\n",
    "    \n",
    "    # Ignore the bottom 100 pixels since they don't add useful information\n",
    "    y_max = HSV_combined.shape[0]-50\n",
    "    HSV_combined[y_max::]=0\n",
    "    #HSV_combined = HSV_combined[0:y_max]\n",
    "                \n",
    "    return HSV_combined\n",
    "\n",
    "# Perspective Transform helper function using the bright road image processed in the gradient section\n",
    "# Passing in M because it is the same for all frames, no need to recalculate\n",
    "def warper(image, M):\n",
    "    offset = 100 # offset for dst points\n",
    "    # Grab the image shape\n",
    "    img_size = (image.shape[1], image.shape[0])\n",
    "\n",
    "    # Warp the image using OpenCV warpPerspective()\n",
    "    binary_warped = cv2.warpPerspective(image, M, img_size)\n",
    "    return binary_warped\n",
    "\n",
    "# Define a class to receive the characteristics of each line detection\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None     \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])]  \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None \n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') \n",
    "        #x values for detected line pixels\n",
    "        self.allx = None  \n",
    "        #y values for detected line pixels\n",
    "        self.ally = None\n",
    "    \n",
    "import sys\n",
    "from numpy import NaN, Inf, arange, isscalar, asarray, array\n",
    "\n",
    "def peakdet(v, delta, x = None):\n",
    "    \"\"\"\n",
    "    Converted from MATLAB script at http://billauer.co.il/peakdet.html\n",
    "    \n",
    "    Returns two arrays\n",
    "    \n",
    "    function [maxtab, mintab]=peakdet(v, delta, x)\n",
    "    %PEAKDET Detect peaks in a vector\n",
    "    %        [MAXTAB, MINTAB] = PEAKDET(V, DELTA) finds the local\n",
    "    %        maxima and minima (\"peaks\") in the vector V.\n",
    "    %        MAXTAB and MINTAB consists of two columns. Column 1\n",
    "    %        contains indices in V, and column 2 the found values.\n",
    "    %      \n",
    "    %        With [MAXTAB, MINTAB] = PEAKDET(V, DELTA, X) the indices\n",
    "    %        in MAXTAB and MINTAB are replaced with the corresponding\n",
    "    %        X-values.\n",
    "    %\n",
    "    %        A point is considered a maximum peak if it has the maximal\n",
    "    %        value, and was preceded (to the left) by a value lower by\n",
    "    %        DELTA.\n",
    "    \n",
    "    % Eli Billauer, 3.4.05 (Explicitly not copyrighted).\n",
    "    % This function is released to the public domain; Any use is allowed.\n",
    "    \n",
    "    \"\"\"\n",
    "    maxtab = []\n",
    "    mintab = []\n",
    "       \n",
    "    if x is None:\n",
    "        x = arange(len(v))\n",
    "    \n",
    "    v = asarray(v)\n",
    "    \n",
    "    if len(v) != len(x):\n",
    "        sys.exit('Input vectors v and x must have same length')\n",
    "    \n",
    "    if not isscalar(delta):\n",
    "        sys.exit('Input argument delta must be a scalar')\n",
    "    \n",
    "    if delta <= 0:\n",
    "        sys.exit('Input argument delta must be positive')\n",
    "    \n",
    "    mn, mx = Inf, -Inf\n",
    "    mnpos, mxpos = NaN, NaN\n",
    "    \n",
    "    lookformax = True\n",
    "    \n",
    "    for i in arange(len(v)):\n",
    "        this = v[i]\n",
    "        if this > mx:\n",
    "            mx = this\n",
    "            mxpos = x[i]\n",
    "        if this < mn:\n",
    "            mn = this\n",
    "            mnpos = x[i]\n",
    "        \n",
    "        if lookformax:\n",
    "            if this < mx-delta:\n",
    "                maxtab.append((mxpos, mx))\n",
    "                mn = this\n",
    "                mnpos = x[i]\n",
    "                lookformax = False\n",
    "        else:\n",
    "            if this > mn+delta:\n",
    "                mintab.append((mnpos, mn))\n",
    "                mx = this\n",
    "                mxpos = x[i]\n",
    "                lookformax = True\n",
    "\n",
    "    return array(maxtab)#, array(mintab)\n",
    "\n",
    "# Pass a binary undistorted and warped image, now look at the histogram to get the lane coordinates\n",
    "\n",
    "def get_lane_pix(binary_warped, left, right, nwindows=9, margin = 100, minpix =50):\n",
    "    \"\"\"\n",
    "    binary_warped = undistarted and birdseye view warped image\n",
    "    left and right = Line() objects\n",
    "    nwindows = number of windows desired to split image in y direction when searching for lane pixels\n",
    "    margin = x margin from window center to search for additinal lane pixels \n",
    "    minpix = min number of pixels needed be considered sufficent to update lane best x-coordinate \n",
    "    Returns an image with the lane lines and road highlighted and with radius of curvature and distance from center\n",
    "    \"\"\"\n",
    "    #print(left.best_fit, right.best_fit)\n",
    "    # Create an output image to draw on and  visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped)) \n",
    "    \n",
    "    # lane.bestx will be used as the center of the window to detect lane pixels\n",
    "    if(left.bestx != None or right.bestx != None): #Use bestx from previous frame\n",
    "        left.detected = True\n",
    "        right.detected = True\n",
    "    else: # No previous bestx, use histogram to find peaks\n",
    "        # Take a histogram of the entire birds eye binary image\n",
    "        histogram = np.sum(binary_warped, axis=0)\n",
    "        midpoint = np.int(histogram.shape[0]/2)\n",
    "        # Find the peak of the left and right halves of the histogram\n",
    "        # These will be the starting point for the left and right lines\n",
    "        #left.bestx = np.max(np.where(histogram[:midpoint]>20000))\n",
    "        #left.bestx = np.argmax(histogram[:midpoint])\n",
    "        #right.bestx = np.min(np.where(histogram[midpoint:]>20000))+midpoint\n",
    "        #right.bestx = np.argmax(histogram[midpoint:])+midpoint\n",
    "        max = peakdet(histogram, 5000)\n",
    "        split1, split2 =np.split(max,2,axis=1)\n",
    "        clearestLinex = split1[np.argmax(split2)]\n",
    "        if clearestLinex < midpoint: # the left lane\n",
    "            left.bestx = clearestLinex\n",
    "            clearestLineOtherx = left.bestx + 700\n",
    "            x_ind = (split1 > clearestLineOtherx-200) & (split1< clearestLineOtherx+200)\n",
    "            if np.array(x_ind==0).all(): # No other lane detected, return function\n",
    "                return out_img, left, right\n",
    "            else:\n",
    "                right.bestx = split1[np.where(split2==np.max(split2[x_ind]))]\n",
    "        else:\n",
    "            right.bestx = clearestLinex\n",
    "            clearestLineOtherx = right.bestx - 700\n",
    "            x_ind = (split1 > clearestLineOtherx-200) & (split1< clearestLineOtherx+200)\n",
    "            if np.array(x_ind==0).all(): # No other lane detected, return function\n",
    "                return out_img, left, right\n",
    "            else:\n",
    "                left.bestx = split1[np.where(split2==np.max(split2[x_ind]))]\n",
    "            \n",
    "    \n",
    "    # Set height of windows\n",
    "    window_height = np.int(binary_warped.shape[0]/nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "\n",
    "    # recent_xfitted capture all best xs captured in current frame\n",
    "    # Why bother storing this? \n",
    "    left.recent_xfitted = []\n",
    "    right.recent_xfitted = []\n",
    "        \n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one and find all lane pixels\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        win_xleft_low = left.bestx - margin\n",
    "        win_xleft_high = left.bestx + margin\n",
    "        win_xright_low = right.bestx - margin\n",
    "        win_xright_high = right.bestx + margin\n",
    "\n",
    "        # Draw the windows on the visualization image\n",
    "        #cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "        #cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 2) \n",
    "        \n",
    "        # Find all none zero pixels within window frame\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) \n",
    "                          & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) \n",
    "                           & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        \n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            # Formerly leftx_current\n",
    "            left.bestx = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "            #temp_left_xfitted.append(left.bestx)\n",
    "            left.recent_xfitted.append(left.bestx)          \n",
    "        if len(good_right_inds) > minpix:        \n",
    "            # Formerly rightx_current\n",
    "            right.bestx = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "            #temp_right_xfitted.append(right.bestx)\n",
    "            right.recent_xfitted.append(right.bestx)\n",
    "   \n",
    "    \n",
    "    left.allx = np.concatenate(left_lane_inds)\n",
    "    right.allx = np.concatenate(right_lane_inds)\n",
    "   \n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left.allx]\n",
    "    lefty = nonzeroy[left.allx] \n",
    "    rightx = nonzerox[right.allx]\n",
    "    righty = nonzeroy[right.allx] \n",
    "    \n",
    "    # If leftx or rightx is 0, then break, won't be able to completely ID road\n",
    "    if len(leftx)==0 or len(rightx)==0:\n",
    "        return out_img, left, right\n",
    "\n",
    "    ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "\n",
    "    # Calculate radisu of curvature\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel\n",
    "    \n",
    "    # Define y-value where we want radius of curvature\n",
    "    # I'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    \n",
    "    y_eval = binary_warped.shape[0]-1\n",
    "    # Fit a second or first order polynomial to each\n",
    "    # 30000 came from the observation that lanes with more than 30000 data points resulted in more accurate lines\n",
    "    # If there are less than 30000 lines, then it might be better to find a straigh line equation\n",
    "    if ( len(left.allx) > 30000): \n",
    "        left.best_fit = np.polyfit(lefty, leftx, 2)\n",
    "        left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "        left.radius_of_curvature = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    elif ( len(left.allx) > 10000 or left.best_fit ==None): # Otherwise use previous fit \n",
    "        left.best_fit = np.polyfit(lefty, leftx, 1)\n",
    "        left.radius_of_curvature = None\n",
    "    if ( len(right.allx) > 30000):\n",
    "        right.best_fit = np.polyfit(righty, rightx, 2)\n",
    "        right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "        right.radius_of_curvature = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    elif ( len(right.allx) > 10000 or right.best_fit ==None):  \n",
    "        right.best_fit = np.polyfit(righty, rightx, 1)\n",
    "        right.radius_of_curvature = None\n",
    "\n",
    "#    if left.best_fit is None:\n",
    "#        left.best_fit = temp_left_best_fit\n",
    "#        right.best_fit = temp_right_best_fit\n",
    "    \n",
    "    # Not sure where to use this downstream\n",
    "    #left.diffs = temp_left_best_fit - left.best_fit\n",
    "    #right.diffs = temp_right_best_fit - right.best_fit\n",
    "    \n",
    "    # Update lane property\n",
    "    #left.best_fit = temp_left_best_fit\n",
    "    #right.best_fit = temp_right_best_fit     \n",
    "    \n",
    "#    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "#    left.radius_of_curvature = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "#    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "#    right.radius_of_curvature = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    \n",
    "    \n",
    "    #visualize\n",
    "    if(len(left.best_fit)>2):\n",
    "        left_fitx = left.best_fit[0]*ploty**2 + left.best_fit[1]*ploty + left.best_fit[2]\n",
    "    else:\n",
    "        left_fitx = left.best_fit[0]*ploty + left.best_fit[1]\n",
    "    if(len(right.best_fit)>2):    \n",
    "        right_fitx = right.best_fit[0]*ploty**2 + right.best_fit[1]*ploty + right.best_fit[2]\n",
    "    else: \n",
    "        right_fitx = right.best_fit[0]*ploty + right.best_fit[1]\n",
    "    #out_img = np.dstack((binary_warped, binary_warped, binary_warped))\n",
    "    #print(\"get_lane_pix out_img\", out_img.shape)\n",
    "    # Paint left and right lanes\n",
    "    # out_img[nonzeroy[left.allx], nonzerox[left.allx]] = [255, 0, 0]\n",
    "    # out_img[nonzeroy[right.allx], nonzerox[right.allx]] = [0, 0, 255]\n",
    "    ## Fill in between the lines\n",
    "    # Get all x values that are in between the lines: \n",
    "    \n",
    "    x_ind =np.array([[x for x in range(binary_warped.shape[1]) if x >= left_fitx[y] and x <= right_fitx[y]] \n",
    "          for y in range(binary_warped.shape[0])])\n",
    "    \n",
    "    for row in range(binary_warped.shape[0]):\n",
    "        out_img[np.asarray(np.ones(len(x_ind[row]))*row, np.int32),np.asarray(x_ind[row])]=[0,255,255]\n",
    "\n",
    "    # Black out non colored pixels\n",
    "    r, g, b = 255, 255, 255\n",
    "    rgb_thresh=[r,g,b]\n",
    "    \n",
    "    thresholds = ((out_img[:,:,0] == rgb_thresh[0]) \\\n",
    "    & (out_img[:,:,1] == rgb_thresh[1]) \\\n",
    "    & (out_img[:,:,2] == rgb_thresh[2])) \\\n",
    "    | ((out_img[:,:,0] < rgb_thresh[0]) \\\n",
    "    & (out_img[:,:,1] < rgb_thresh[1]) \\\n",
    "    & (out_img[:,:,2] < rgb_thresh[2]))\n",
    "    out_img[thresholds] = [0,0,0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #plt.imshow(out_img)\n",
    "    #plt.plot(left_fitx, ploty, color='yellow')\n",
    "    #plt.plot(right_fitx, ploty, color='yellow')\n",
    "    #plt.xlim(0, binary_warped.shape[1])\n",
    "    #plt.ylim(binary_warped.shape[0], 0)\n",
    "    return out_img, left,right\n",
    "\n",
    "def weighted_img(img, initial_img, α=0.8, β=1.0, λ=0.):\n",
    "    \"\"\"\n",
    "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    \n",
    "    `initial_img` should be the image before any processing.\n",
    "    \n",
    "    The result image is computed as follows:\n",
    "    \n",
    "    initial_img * α + img * β + λ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    # Add back the bottom frame we removed\n",
    "    # Additional extracted vectors:\n",
    "    #add_y = np.zeros((initial_img.shape[0]-img.shape[0],img.shape[1]),dtype=np.uint8)\n",
    "    #img = np.append(img,np.dstack((add_y, add_y, add_y)),axis = 0)\n",
    "    \n",
    "    return cv2.addWeighted(initial_img, α, img, β, λ)\n",
    "\n",
    "    \n",
    "def get_vehiclepos(img_width, left_lane, right_lane):\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    xm_per_pix = 3.7/700# meters per pixel in x dimension\n",
    "    \n",
    "    # get the first window\n",
    "    lane_center = (left_lane.bestx+ right_lane.bestx)/2\n",
    "    img_center = img_width/2\n",
    "    \n",
    "    # If lane_center > img_center, then to the right, and is X meters right of lane\n",
    "    diff = img_center - lane_center\n",
    "    diff = diff*xm_per_pix\n",
    "    \n",
    "    left_lane.line_base_pos = diff\n",
    "    right_lane.line_base_pos = diff\n",
    "    \n",
    "    return diff, left_lane, right_lane\n",
    "\n",
    "\n",
    "# write radius of curvature and vehicle position on \n",
    "def write_text(img, left, right):\n",
    "    img_shape = img.shape\n",
    "    if ((left.radius_of_curvature == None) and (right.radius_of_curvature==None)):\n",
    "        radius = 0\n",
    "    elif left.radius_of_curvature == None:\n",
    "        radius = right.radius_of_curvature\n",
    "    elif right.radius_of_curvature == None:\n",
    "        radius = left.radius_of_curvature\n",
    "    else:\n",
    "        radius = np.mean([left.radius_of_curvature, right.radius_of_curvature])\n",
    "\n",
    "    pos, left, right = get_vehiclepos(img_shape[1],left,right)\n",
    "    \n",
    "    if pos >= 0:\n",
    "        direction = \"right\"\n",
    "    else:\n",
    "        direction = \"left\"\n",
    "    lane_pos = np.abs(pos)\n",
    "\n",
    "    radius_text = \"Radius of Curvature = %.2f (m)\" % radius\n",
    "    lane_pos_text = \"Vehicle is %.2f meters %s of center\" % (lane_pos,direction) \n",
    "    cv2.putText(img, radius_text, (50,50),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "    cv2.putText(img, lane_pos_text, (50,100),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve all images\n",
    "# This loop might take some time since it is iterating and processing all camera_cal images\n",
    "# Need to run this to get imgpoints and objpoints\n",
    "import glob\n",
    "import os\n",
    "images = glob.glob('./camera_cal/calibration*.jpg')\n",
    "\n",
    "# Number of corners inside the checker image in X and Y dimensions\n",
    "nx = 9# Manually counted corners within the image\n",
    "ny = 6# Manually counted corners within the image\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "output_folder = './output_images'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Camera Calibration variables to fix optical distortion from lens\n",
    "objpoints = [] #3D points in real world space\n",
    "imgpoints = [] #2D points on image\n",
    "\n",
    "# Returns an array of object points\n",
    "# Z dim will always be 0\n",
    "objp = np.zeros((nx*ny, 3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:nx,0:ny].T.reshape(-1,2) #x, y coordinates\n",
    "\n",
    "# Read in all images in camera_cal folder\n",
    "# Collect imgpoints to use to undistort\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "\n",
    "    # Step 1: Calibrate Camera \n",
    "    # We will use the camera calibrations detected from the checker images to determine the \n",
    "    # intrinsic camera properties which we will use downstream for our car camera images to \n",
    "    # undistort the video stream.\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx,ny), None)\n",
    "\n",
    "    # If found, draw corners\n",
    "    if ret == True:\n",
    "        # Append relevant points to use later on when undistorting images\n",
    "        imgpoints.append(corners)\n",
    "        objpoints.append(objp)\n",
    "        checkered_img = img.copy()\n",
    "\n",
    "        # Draw and display the corners\n",
    "        #cv2.drawChessboardCorners(checkered_img, (nx, ny), corners, ret)\n",
    "        \n",
    "        # Save id'd points to output folder\n",
    "        #save_fname = os.path.join(output_folder,'chessBoardCorners_' + os.path.basename(fname))\n",
    "        #cv2.imwrite(save_fname, checkered_img)\n",
    "\n",
    "# Undistort Image\n",
    "#for fname in images:\n",
    "#    img = cv2.imread(fname)\n",
    "\n",
    "    # Step 2: Correct for Distortion \n",
    "#    undistorted = cal_undistort(img, objpoints, imgpoints)\n",
    "#    save_fname = os.path.join(output_folder,'undistorted_' + os.path.basename(fname))\n",
    "\n",
    "#    cv2.imwrite(save_fname, undistorted)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find average diestances between peaks\n",
    "images = glob.glob('./C/frame*.JPG')\n",
    "## Project Variables\n",
    "#left_x = []\n",
    "#right_x = []\n",
    "#diff = []\n",
    "\n",
    "## Challenge\n",
    "#left_Cx = []\n",
    "#right_Cx = []\n",
    "#diff_C = []\n",
    "\n",
    "## Harder Challenge\n",
    "left_HCx = []\n",
    "right_HCx = []\n",
    "diff_HC = []\n",
    "\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    left = Line()\n",
    "    right = Line()\n",
    "    img_size = [img.shape[1],img.shape[0]]\n",
    "    src = np.float32(\n",
    "        [[(0 + 100), img_size[1]],\n",
    "        [(img_size[0]/2) - 150, (img_size[1]*3 / 5)+75],\n",
    "        [(img_size[0]/2) + 150, (img_size[1]*3 / 5)+75],\n",
    "        [(img_size[0] - 100), (img_size[1])]])\n",
    "\n",
    "    # For destination points, I'm arbitrarily choosing some points to be\n",
    "    # a nice fit for displaying our warped result \n",
    "    # again, not exact, but close enough for our purposes\n",
    "    dst = np.float32(\n",
    "        [[(img_size[0] / 6), img_size[1]],\n",
    "        [(img_size[0] / 6), 0],\n",
    "        [(img_size[0] * 5 / 6), 0],\n",
    "        [(img_size[0] * 5 / 6), img_size[1]]])\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    # First undistort radial distortions\n",
    "    undistort = cal_undistort(img, objpoints, imgpoints)\n",
    "    #print(\"undistort.shape\", undistort.shape)\n",
    "\n",
    "    # Run through a guassian smoothing\n",
    "    gaussian = cv2.GaussianBlur(undistort,(7,7),0)\n",
    "    #print(\"gaussian.shape\", gaussian.shape)\n",
    "\n",
    "    # Extract R & G colors and HSV Value channel\n",
    "    gradient = extract_lane_gradient(gaussian,thresh=(20,255),dir_thresh=(0.7,1.3),RGB=1)\n",
    "    #print(\"gradient.shape\", gradient.shape)\n",
    "\n",
    "    # Get lane colored birdseye perspective\n",
    "    #lane = gradient.copy()\n",
    "    binary_warped = warper(gradient, M)\n",
    "    #print(\"binary_warped.shape\", binary_warped.shape)\n",
    "\n",
    "    histogram = np.sum(binary_warped, axis=0)\n",
    "    midpoint = int(binary_warped.shape[1]/2)\n",
    "    left_HCx.append( np.argmax(histogram[:midpoint]))\n",
    "    \n",
    "    right_HCx.append(np.argmax(histogram[midpoint:]))\n",
    "from scipy import stats\n",
    "diff_HC =left_HCx-right_HCx\n",
    "print(np.mean(diff_HC),stats.mode(diff_HC),np.median(diff_HC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "right_HCx = np.ones(len(right_HCx))*midpoint+np.array(right_HCx)\n",
    "diff_HC =np.array(left_HCx)-np.array(right_HCx)\n",
    "print(len(right_HCx),len(left_HCx))\n",
    "print(np.mean(diff_HC),stats.mode(diff_HC),np.median(diff_HC))\n",
    "print(left_HCx[0],right_HCx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of an original input image vs the undistorted output the above code generated. Notice how you can see some minor radial distortion on the right hand side where the edge of the board curves inside. While the right hand side of the undistorted image is straighter than the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display sample distorted and undistorted images\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,3))\n",
    "f.tight_layout()\n",
    "img = cv2.imread('./camera_cal/calibration4.jpg')\n",
    "\n",
    "undistorted = cv2.imread('./output_images/undistorted_calibration4.jpg')\n",
    "ax1.imshow(img)\n",
    "ax1.set_title('Original Image', fontsize=20)\n",
    "ax2.imshow(undistorted)\n",
    "ax2.set_title('Undistorted', fontsize=20)\n",
    "save_fname = os.path.join(output_folder,'orig_vs_undistorted_' + os.path.basename(fname))\n",
    "f.savefig(save_fname, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replicate with a light road image\n",
    "#fnameLte = './test_images/test4.jpg'\n",
    "fnameLte = 'frame1000.jpg'\n",
    "imgLte_BGR = cv2.imread(fnameLte)\n",
    "imgLte_RGB = cv2.cvtColor(imgLte_BGR, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Correct for Distortion \n",
    "undistorted_lte = cal_undistort(imgLte_RGB, objpoints, imgpoints)\n",
    "# Save image for use downstream\n",
    "save_fname = os.path.join(output_folder,'undistorted_' + os.path.basename(fnameLte))\n",
    "cv2.imwrite(save_fname, undistorted_lte)\n",
    "\n",
    "# Compare before and after\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,10))\n",
    "f.tight_layout()\n",
    "ax1.imshow(imgLte_RGB)\n",
    "ax1.set_title('Original Image', fontsize=20)\n",
    "ax2.imshow(undistorted_lte)\n",
    "ax2.set_title('Undistorted', fontsize=20)\n",
    "save_fname = os.path.join(output_folder,'orig_vs_undistorted_' + os.path.basename(fnameLte))\n",
    "f.savefig(save_fname, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"find-lane-lines\">Find Lane Lines</a>\n",
    "Now that we have adjusted for some of the optical distortions caused my the camera lens, we can begin manipulating the images to find the lane lines. \n",
    "\n",
    "Lane lines can be hard to detect in different lighting conditions and when they are different colors. Below we observe what color maps and gradients best bring out yellow and white lane lines in shadows and in the light.\n",
    "\n",
    "#### <a name=\"id-best-color-maps\">ID Best Color Maps</a>\n",
    "Once we extract the specific channel we want to view, I run the image through a threshold filter to only pick up on pixels of a certain range of threshold. We had to modify the threshold value for the light vs dark road images. As the pipeline gets further refined downstream, these thresholds would have to be automatically adjusted as the lighting conditions change on the road.\n",
    "\n",
    "**RGB color map on bright & dark roads:** \n",
    "\n",
    "*Analysis:* \n",
    "* The *B* (blue) values cannot capture yellow lines well.\n",
    "* The *R* value seems to capture yellow and white lane lines well in both light and dark roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient Thresholds\n",
    "threshLte = (140,255)\n",
    "threshDrk = (40,100)\n",
    "\n",
    "# Visualize RGB channels of undistorted test4.jpg-- \"light road\" image\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(undistorted_lte)\n",
    "ax1.set_title('RGB: Light Road', fontsize=20)\n",
    "r_binary = get_binary(undistorted_lte[:,:,(0)], thresh=threshLte)\n",
    "ax2.imshow(r_binary, cmap = 'gray')\n",
    "ax2.set_title('R', fontsize=20)\n",
    "g_binary = get_binary(undistorted_lte[:,:,(1)], thresh=threshLte)\n",
    "ax3.imshow(g_binary, cmap = 'gray')\n",
    "ax3.set_title('G', fontsize=20)\n",
    "g_binary = get_binary(undistorted_lte[:,:,(2)], thresh=threshLte)\n",
    "ax4.imshow(g_binary, cmap = 'gray')\n",
    "ax4.set_title('B', fontsize=20)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "# Save dark image \"additionalDarkRoadTest.jpg\" for use downstream\n",
    "fnameDrk = './test_images/additionalDarkRoadTest.jpg'\n",
    "\n",
    "# cv2 reads in image as BGR\n",
    "imgDrk_BGR = cv2.imread(fnameDrk)\n",
    "imgDrk_RGB = cv2.cvtColor(imgDrk_BGR, cv2.COLOR_BGR2RGB)\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(imgDrk_RGB)\n",
    "ax1.set_title('RGB: Dark Image', fontsize=20)\n",
    "r_binary = get_binary(imgDrk_RGB[:,:,(0)], thresh=threshDrk)\n",
    "ax2.imshow(r_binary, cmap = 'gray')\n",
    "g_binary = get_binary(imgDrk_RGB[:,:,(1)], thresh=threshDrk)\n",
    "ax3.imshow(g_binary, cmap = 'gray')\n",
    "g_binary = get_binary(imgDrk_RGB[:,:,(2)], thresh=threshDrk)\n",
    "ax4.imshow(g_binary, cmap = 'gray')\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HSV color map on bright & dark roads:** HSV stands for hue, saturation, and value. Read more on how the color representation is calculated [here](http://codeitdown.com/hsl-hsb-hsv-color/)\n",
    "\n",
    "*Analysis:* \n",
    "* The *value* field seems to be best at identifying white and yellow lines for both well lit and dark images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshLte=(50,255)\n",
    "imgLte_HSV = cv2.cvtColor(undistorted_lte,cv2.COLOR_RGB2HSV)\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(imgLte_HSV)\n",
    "ax1.set_title('HVS: Light Road', fontsize=20)\n",
    "\n",
    "h_binary = get_binary(imgLte_HSV[:,:,(0)],thresh=threshLte)\n",
    "ax2.imshow(h_binary, cmap='gray')\n",
    "ax2.set_title('H', fontsize=20)\n",
    "\n",
    "v_binary = get_binary(imgLte_HSV[:,:,(2)],thresh=threshLte)\n",
    "ax3.imshow(v_binary, cmap='gray')\n",
    "ax3.set_title('V', fontsize=20)\n",
    "\n",
    "s_binary = get_binary(imgLte_HSV[:,:,(1)],thresh=threshLte)\n",
    "ax4.imshow(s_binary, cmap='gray')\n",
    "ax4.set_title('S', fontsize=20)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "# Dark Image\n",
    "imgDrk_HSV = cv2.cvtColor(imgDrk_BGR,cv2.COLOR_BGR2HSV)\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(imgDrk_HSV)\n",
    "ax1.set_title('HVS: Dark Road', fontsize=20)\n",
    "\n",
    "h_binary = get_binary(imgDrk_HSV[:,:,(0)],thresh=threshDrk)\n",
    "ax2.imshow(h_binary, cmap='gray')\n",
    "\n",
    "v_binary = get_binary(imgDrk_HSV[:,:,(2)],thresh=threshDrk)\n",
    "ax3.imshow(v_binary, cmap='gray')\n",
    "\n",
    "s_binary = get_binary(imgDrk_HSV[:,:,(1)],thresh=threshDrk)\n",
    "ax4.imshow(s_binary, cmap='gray')\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HLS color map on bright and dark road:** HLS stands for hue, lightness, and saturation. \n",
    "\n",
    "*Analysis:*\n",
    "* The *saturation* field is able to capture both white and yellow lines. This is because the saturation value in *HLS* is depenedent on the associated lightness of the pixel, and not just its intensity. \n",
    "* The *lightness* dimension doesn't bring out the lane lines as much as the *value* dimension in the *HSV* space. This might be because *lightness* is an average of the brightest and darkest RGB values, while the *value* is simply the brightest value. \n",
    "* Using the existing light (220,255) and dark (40,100) gradient thresholds, the HLS slices are less able to pick up lane lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HLS Analysis\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "imgLte_HLS = cv2.cvtColor(undistorted_lte, cv2.COLOR_RGB2HLS)\n",
    "ax1.imshow(imgLte_HLS)\n",
    "ax1.set_title('HLS: Light Road', fontsize=20)\n",
    "\n",
    "h_binary = get_binary(imgLte_HLS[:,:,(0)],thresh=threshLte)\n",
    "ax2.imshow(h_binary, cmap='gray')\n",
    "ax2.set_title('H', fontsize=20)\n",
    "\n",
    "l_binary = get_binary(imgLte_HLS[:,:,(1)],thresh=threshLte)\n",
    "ax3.imshow(l_binary, cmap='gray')\n",
    "ax3.set_title('L', fontsize=20)\n",
    "\n",
    "s_binary = get_binary(imgLte_HLS[:,:,(2)],thresh=threshLte)\n",
    "ax4.imshow(s_binary, cmap='gray')\n",
    "ax4.set_title('S', fontsize=20)\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "# Dark road\n",
    "imgDrk_HLS = cv2.cvtColor(imgDrk_BGR,cv2.COLOR_BGR2HLS)\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24, 9))\n",
    "f.tight_layout()\n",
    "ax1.imshow(imgDrk_HLS)\n",
    "ax1.set_title('HLS: Dark Road', fontsize=20)\n",
    "\n",
    "h_binary = get_binary(imgDrk_HLS[:,:,(0)],thresh=threshDrk)\n",
    "ax2.imshow(h_binary, cmap='gray')\n",
    "\n",
    "l_binary = get_binary(imgDrk_HLS[:,:,(1)],thresh=threshDrk)\n",
    "ax3.imshow(l_binary, cmap='gray')\n",
    "\n",
    "s_binary = get_binary(imgDrk_HLS[:,:,(2)],thresh=threshDrk)\n",
    "ax4.imshow(s_binary, cmap='gray')\n",
    "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Color map conclusions:**\n",
    "* Specifically to bring out white and yellow lanes, we should ignore the blue values\n",
    "* Within the *HVS* color map, the *value* field is best in both bright and dark lighting situations at bringing out white and yellow lanes.\n",
    "* WIthin the *HLS* color map, the *saturation* field brings out lane lines best in bright images, but is not as reliable in detecting white lanes in dark lighting. \n",
    "\n",
    "Using these observations, it seems like we could maximize our lane detection if we do not use the blue channel and if we use the *value* field in the HVS color map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### <a name=\"find-sobel-gradients\">Find Sobel Gradients</a>\n",
    "The sobel operator is a [convolution](http://setosa.io/ev/image-kernels/) matrix which is used to approximate the changes in pixel intensity (gradient). A 3x3 kernel looks like: \n",
    "\n",
    "$G_x = \n",
    "\\left[ \\begin{array}{cccc}\n",
    "1 & 0 & -1 \\\\\n",
    "2 & 0 & -2 \\\\\n",
    "1 & 0 & -1 \\\\ \\end{array} \\right] * A$\n",
    "\n",
    "$G_y = \n",
    "\\left[ \\begin{array}{cccc}\n",
    "1 & 2 & 1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-1 & -2 & -1 \\\\ \\end{array} \\right] * A$\n",
    "\n",
    "The key properties of the sobel operator is that it is depending on if you're look at the x or y gradient, the kernel has symmetric but opposite direction values in the first half/top half of the matrix to the second half/bottom half. This causes the convolution to return 0 if the pixels in the surrounding area are similar and will return larger values otherwise, consequently highlighting edges. Below we look at the edges detected using the sobel operators on a normal image, then on an image that has been enhanced leveraging our observations from analysis on different color maps.\n",
    "\n",
    "We use the sobel operator here rather than the canny edge detection algorithm so that we can focus on finding edges of a certain direction (more vertical rather than horizontal) so that we could customize that edge detection process to only return edges most likely to be lane lines. \n",
    "___\n",
    "**Sobel Operators on original image**\n",
    "Below we collect different gradients. In the next section, we will compare gradients extracted from the original image vs gradients extracted from an image with the color map manipulated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sobel Operator on Image with manipulated Color map**\n",
    "The cvtColor documentation states that the alorithm used to convert RGB to color is\n",
    "$$Y = 0.299 R + 0.587 G + 0.114 B$$\n",
    "Knowing this, we will replace the $B$ value with $R$, since the red value seemed to bring out the lane lines best from the above RGB on dark and bright road sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(undistorted_lte,cv2.COLOR_RGB2GRAY)\n",
    "gray[gray<140]=140\n",
    "plt.imshow(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compare a few different combinations of gradients. \n",
    "# Goal is to select combination that best highlights lane lines\n",
    "# Reset threshLte otherwise threshold too high for these color maps\n",
    "threshLte=(20,255)\n",
    "ksize =15\n",
    "### Set 1: Gradients of Original Image\n",
    "# Image 1: Original test4 image\n",
    "# Image 2: Grayscale\n",
    "pre_gray = undistorted_lte.copy()\n",
    "pre_gray[:,:,2]=pre_gray[:,:,0]\n",
    "gray = cv2.cvtColor(pre_gray,cv2.COLOR_RGB2GRAY)\n",
    "gray[gray<200]=0\n",
    "\n",
    "# Image 3: x Gradient\n",
    "gradx = abs_sobel_thresh(gray, orient='x', sobel_kernel=ksize, thresh=threshLte)\n",
    "\n",
    "# Image 4: y Gradient\n",
    "grady = abs_sobel_thresh(gray, orient='y', sobel_kernel=ksize, thresh=threshLte)\n",
    "\n",
    "# Image 5: X & Y Gradient magnitude\n",
    "mag_binary = mag_thresh(gray, sobel_kernel=ksize, thresh=threshLte)\n",
    "\n",
    "# Image 6: Only gradients that are  almost verticle-- most likely to be lane lines\n",
    "dir_binary = dir_threshold(gray, sobel_kernel=ksize, thresh=(0.7, 1.3))\n",
    "\n",
    "# Image 7: All gradients combied\n",
    "# Combine all gradients\n",
    "combined = np.zeros_like(dir_binary)\n",
    "combined[((gradx == 255) & (grady == 255)) | ((mag_binary == 255) & (dir_binary == 255))] = 255\n",
    "\n",
    "###---------------------------------------------\n",
    "### Set 2: Gradients of image with only Red & Green channel values \n",
    "# Image 1: Copy R values over B channel\n",
    "img_RG = undistorted_lte.copy()\n",
    "img_RG[:,:,2] = img_RG[:,:,0]\n",
    "\n",
    "# Image 2: Grayscale of only R&G Values\n",
    "gray_RG = cv2.cvtColor(img_RG,cv2.COLOR_RGB2GRAY)\n",
    "gray_RG[gray_RG<200]=0\n",
    "\n",
    "# Image 3: x Gradient of only R&G Values\n",
    "gradx_RG = abs_sobel_thresh(gray_RG, orient='x', sobel_kernel=ksize, thresh=threshLte)\n",
    "\n",
    "# Image 4: y Gradient of only R&G Values\n",
    "grady_RG = abs_sobel_thresh(gray_RG, orient='y', sobel_kernel=ksize, thresh=threshLte)\n",
    "\n",
    "# Image 5: X & Y Gradient magnitude of only R&G Values\n",
    "mag_binary_RG = mag_thresh(gray_RG, sobel_kernel=ksize, thresh=threshLte)\n",
    "\n",
    "# Image 6: Only gradients that are  almost verticle-- most likely to be lane lines\n",
    "dir_binary_RG = dir_threshold(gray_RG, sobel_kernel=ksize, thresh=(0.7, 1.3))\n",
    "\n",
    "# Image 7: All gradients combined of only R&G Values \n",
    "# Combine all gradients\n",
    "combined_RG = np.zeros_like(dir_binary_RG)\n",
    "combined_RG[((gradx_RG == 255) & (grady_RG == 255)) | ((mag_binary_RG == 255) & (dir_binary_RG == 255))] = 255\n",
    "\n",
    "###---------------------------------------------\n",
    "### Set 3: Gradients of V channel of HSV image converted from image with only Red & Green channel values\n",
    "# Image 1: Only R&G Values\n",
    "img_RG_V = img_RG.copy()\n",
    "img_RG_V = cv2.cvtColor(img_RG_V, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "#img_RG_V[:,:,0]= img_RG_V[:,:,2]\n",
    "#img_RG_V[:,:,1]= img_RG_V[:,:,2]\n",
    "\n",
    "# Image 2: Grayscale of only R&G and V value\n",
    "gray_RG_V = img_RG_V[:,:,0]\n",
    "\n",
    "# Image 3: x Gradient of only R&G Values\n",
    "gradx_RG_V = abs_sobel_thresh(gray_RG_V, orient='x', sobel_kernel=ksize, thresh=threshLte)\n",
    "\n",
    "# Image 4: y Gradient of only R&G Values\n",
    "grady_RG_V = abs_sobel_thresh(gray_RG_V, orient='y', sobel_kernel=ksize, thresh=threshLte)\n",
    "\n",
    "# Image 5: X & Y Gradient magnitude of only R&G Values\n",
    "mag_binary_RG_V = mag_thresh(gray_RG_V, sobel_kernel=ksize, thresh=threshLte)\n",
    "\n",
    "# Image 6: Only gradients that are  almost verticle-- most likely to be lane lines\n",
    "dir_binary_RG_V = dir_threshold(gray_RG_V, sobel_kernel=ksize, thresh=(0.7, np.pi/2))\n",
    "\n",
    "# Image 7: All gradients combined of only R&G Values \n",
    "# Combine all gradients\n",
    "combined_RG_V = np.zeros_like(dir_binary_RG_V)\n",
    "combined_RG_V[((gradx_RG_V == 255) & (grady_RG_V == 255)) | ((mag_binary_RG_V == 255) & (dir_binary_RG_V == 255))] = 255\n",
    "\n",
    "\n",
    "#plot all original images\n",
    "fig = plt.figure(figsize = (18,30))\n",
    "ax1 = fig.add_subplot(731)\n",
    "ax1.set_title('Original Image', fontsize=20)\n",
    "ax1.imshow(undistorted_lte)\n",
    "ax4 = fig.add_subplot(734)\n",
    "ax4.set_title('Grayscale', fontsize=20)\n",
    "ax4.imshow(gray, cmap='gray')\n",
    "ax7 = fig.add_subplot(737)\n",
    "ax7.set_title('x Gradient', fontsize=20)\n",
    "ax7.imshow(gradx, cmap = 'gray')\n",
    "ax10 = fig.add_subplot(7,3,10)\n",
    "ax10.set_title('Y Gradient', fontsize=20)\n",
    "ax10.imshow(grady, cmap = 'gray')\n",
    "ax13 = fig.add_subplot(7,3,13)\n",
    "ax13.set_title('X & Y Gradient Magnitude', fontsize=20)\n",
    "ax13.imshow(mag_binary,cmap='gray')\n",
    "ax16 = fig.add_subplot(7,3,16)\n",
    "ax16.set_title('Vertical Gradients', fontsize=20)\n",
    "ax16.imshow(dir_binary,cmap='gray')\n",
    "ax19 = fig.add_subplot(7,3,19)\n",
    "ax19.set_title('All Gradients Combined', fontsize=20)\n",
    "ax19.imshow(combined,cmap='gray')\n",
    "\n",
    "#plot all images with manipulated color spalce\n",
    "ax2 = fig.add_subplot(732)\n",
    "ax2.set_title('Only R & G values', fontsize=20)\n",
    "ax2.imshow(img_RG)\n",
    "ax5 = fig.add_subplot(735)\n",
    "ax5.imshow(gray_RG, cmap='gray')\n",
    "ax8 = fig.add_subplot(738)\n",
    "ax8.imshow(gradx_RG, cmap = 'gray')\n",
    "ax11 = fig.add_subplot(7,3,11)\n",
    "ax11.imshow(grady_RG, cmap = 'gray')\n",
    "ax14 = fig.add_subplot(7,3,14)\n",
    "ax14.imshow(mag_binary_RG,cmap='gray')\n",
    "ax17 = fig.add_subplot(7,3,17)\n",
    "ax17.imshow(dir_binary_RG,cmap='gray')\n",
    "ax20 = fig.add_subplot(7,3,20)\n",
    "ax20.imshow(combined_RG,cmap='gray')\n",
    "\n",
    "#plot all images with manipulated color spalce\n",
    "ax3 = fig.add_subplot(733)\n",
    "ax3.set_title('V (HSV) from RG channels', fontsize=20)\n",
    "ax3.imshow(img_RG_V)\n",
    "ax6 = fig.add_subplot(736)\n",
    "ax6.imshow(gray_RG_V, cmap='gray')\n",
    "ax9 = fig.add_subplot(739)\n",
    "ax9.imshow(gradx_RG_V, cmap = 'gray')\n",
    "ax12 = fig.add_subplot(7,3,12)\n",
    "ax12.imshow(grady_RG_V, cmap = 'gray')\n",
    "ax15 = fig.add_subplot(7,3,15)\n",
    "ax15.imshow(mag_binary_RG_V,cmap='gray')\n",
    "ax18 = fig.add_subplot(7,3,18)\n",
    "ax18.imshow(dir_binary_RG_V,cmap='gray')\n",
    "ax21 = fig.add_subplot(7,3,21)\n",
    "ax21.imshow(combined_RG_V,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat above with image of road with dark lighting\n",
    "###### Find best threshold parameters\n",
    "threshDr = (20,255)\n",
    "# Image 1: Original (imgDrk_RGB)\n",
    "# Image 2: Grayscale\n",
    "gray_Drk = cv2.cvtColor(imgDrk_RGB,cv2.COLOR_RGB2GRAY)\n",
    "gray_Drk[gray_Drk<140]=0\n",
    "\n",
    "# Image 3: x Gradient\n",
    "gradx_Drk = abs_sobel_thresh(gray_Drk, orient='x', sobel_kernel=ksize, thresh=threshDrk)\n",
    "\n",
    "# Image 4: y Gradient\n",
    "grady_Drk = abs_sobel_thresh(gray_Drk, orient='y', sobel_kernel=ksize, thresh=threshDrk)\n",
    "\n",
    "# Image 5: X & Y Gradient magnitude\n",
    "mag_binary_Drk = mag_thresh(gray_Drk, sobel_kernel=ksize, thresh=threshDrk)\n",
    "\n",
    "# Image 6: Only gradients that are  almost verticle-- most likely to be lane lines\n",
    "dir_binary_Drk = dir_threshold(gray_Drk, sobel_kernel=ksize, thresh=(0.7, 1.3))\n",
    "\n",
    "# Image 7: All gradients combied\n",
    "# Combine all gradients\n",
    "combined_Drk = np.zeros_like(dir_binary_Drk)\n",
    "combined_Drk[((gradx_Drk == 255) & (grady_Drk == 255)) | ((mag_binary_Drk == 255) & (dir_binary_Drk == 255))] = 255\n",
    "\n",
    "###---------------------------------------------\n",
    "### Set 2: Gradients of image with only Red & Green channel values \n",
    "# Image 1: Copy R values over B channel\n",
    "img_RG_Drk = imgDrk_RGB.copy()\n",
    "img_RG_Drk[:,:,2] = img_RG_Drk[:,:,0]\n",
    "\n",
    "# Image 2: Grayscale of only R&G Values\n",
    "gray_RG_Drk = cv2.cvtColor(img_RG_Drk,cv2.COLOR_RGB2GRAY)\n",
    "gray_RG_Drk[gray_RG_Drk<200]=0\n",
    "\n",
    "# Image 3: x Gradient of only R&G Values\n",
    "gradx_RG_Drk = abs_sobel_thresh(gray_RG_Drk, orient='x', sobel_kernel=ksize, thresh=threshDrk)\n",
    "\n",
    "# Image 4: y Gradient of only R&G Values\n",
    "grady_RG_Drk = abs_sobel_thresh(gray_RG_Drk, orient='y', sobel_kernel=ksize, thresh=threshDrk)\n",
    "\n",
    "# Image 5: X & Y Gradient magnitude of only R&G Values\n",
    "mag_binary_RG_Drk = mag_thresh(gray_RG_Drk, sobel_kernel=ksize, thresh=threshDrk)\n",
    "\n",
    "# Image 6: Only gradients that are  almost verticle-- most likely to be lane lines\n",
    "dir_binary_RG_Drk = dir_threshold(gray_RG_Drk, sobel_kernel=ksize, thresh=(0.7, 1.3))\n",
    "\n",
    "# Image 7: All gradients combined of only R&G Values \n",
    "# Combine all gradients\n",
    "combined_RG_Drk = np.zeros_like(dir_binary_RG_Drk)\n",
    "combined_RG_Drk[((gradx_RG_Drk == 255) & (grady_RG_Drk == 255)) \n",
    "                | ((mag_binary_RG_Drk == 255) & (dir_binary_RG_Drk == 255))] = 255\n",
    "\n",
    "###---------------------------------------------\n",
    "### Set 3: Gradients of V channel of HSV image converted from image with only Red & Green channel values\n",
    "# Image 1: Only R&G Values\n",
    "img_RG_V_Drk = img_RG_Drk.copy()\n",
    "img_RG_V_Drk = cv2.cvtColor(img_RG_V_Drk, cv2.COLOR_RGB2HSV)\n",
    "img_RG_V_Drk[:,:,0]= img_RG_V_Drk[:,:,2]\n",
    "img_RG_V_Drk[:,:,1]= img_RG_V_Drk[:,:,2]\n",
    "\n",
    "# Image 2: Grayscale of only R&G and V value\n",
    "gray_RG_V_Drk = img_RG_V_Drk[:,:,2]\n",
    "\n",
    "# Image 3: x Gradient of only R&G Values\n",
    "gradx_RG_V_Drk = abs_sobel_thresh(gray_RG_V_Drk, orient='x', sobel_kernel=ksize, thresh=threshDrk)\n",
    "\n",
    "# Image 4: y Gradient of only R&G Values\n",
    "grady_RG_V_Drk = abs_sobel_thresh(gray_RG_V_Drk, orient='y', sobel_kernel=ksize, thresh=threshDrk)\n",
    "\n",
    "# Image 5: X & Y Gradient magnitude of only R&G Values\n",
    "mag_binary_RG_V_Drk = mag_thresh(gray_RG_V_Drk, sobel_kernel=ksize, thresh=threshDrk)\n",
    "\n",
    "# Image 6: Only gradients that are  almost verticle-- most likely to be lane lines\n",
    "dir_binary_RG_V_Drk = dir_threshold(gray_RG_V_Drk, sobel_kernel=ksize, thresh=(0.7, np.pi/2))\n",
    "\n",
    "# Image 7: All gradients combined of only R&G Values \n",
    "# Combine all gradients\n",
    "combined_RG_V_Drk = np.zeros_like(dir_binary_RG_V_Drk)\n",
    "combined_RG_V_Drk[((gradx_RG_V_Drk == 255) & (grady_RG_V_Drk == 255)) \n",
    "                  | ((mag_binary_RG_V_Drk == 255) & (dir_binary_RG_V_Drk == 255))] = 255\n",
    "\n",
    "\n",
    "#plot all original images\n",
    "fig = plt.figure(figsize = (18,30))\n",
    "ax1 = fig.add_subplot(731)\n",
    "ax1.set_title('Original Image', fontsize=20)\n",
    "ax1.imshow(imgDrk_RGB)\n",
    "ax4 = fig.add_subplot(734)\n",
    "ax4.set_title('Grayscale', fontsize=20)\n",
    "ax4.imshow(gray_Drk, cmap='gray')\n",
    "ax7 = fig.add_subplot(737)\n",
    "ax7.set_title('x Gradient', fontsize=20)\n",
    "ax7.imshow(gradx_Drk, cmap = 'gray')\n",
    "ax10 = fig.add_subplot(7,3,10)\n",
    "ax10.set_title('Y Gradient', fontsize=20)\n",
    "ax10.imshow(grady_Drk, cmap = 'gray')\n",
    "ax13 = fig.add_subplot(7,3,13)\n",
    "ax13.set_title('X & Y Gradient Magnitude', fontsize=20)\n",
    "ax13.imshow(mag_binary_Drk,cmap='gray')\n",
    "ax16 = fig.add_subplot(7,3,16)\n",
    "ax16.set_title('Vertical Gradients', fontsize=20)\n",
    "ax16.imshow(dir_binary_Drk,cmap='gray')\n",
    "ax19 = fig.add_subplot(7,3,19)\n",
    "ax19.set_title('All Gradients Combined', fontsize=20)\n",
    "ax19.imshow(combined_Drk,cmap='gray')\n",
    "\n",
    "#plot all images with manipulated color spalce\n",
    "ax2 = fig.add_subplot(732)\n",
    "ax2.set_title('Only R & G values', fontsize=20)\n",
    "ax2.imshow(img_RG_Drk)\n",
    "ax5 = fig.add_subplot(735)\n",
    "ax5.imshow(gray_RG_Drk, cmap='gray')\n",
    "ax8 = fig.add_subplot(738)\n",
    "ax8.imshow(gradx_RG_Drk, cmap = 'gray')\n",
    "ax11 = fig.add_subplot(7,3,11)\n",
    "ax11.imshow(grady_RG_Drk, cmap = 'gray')\n",
    "ax14 = fig.add_subplot(7,3,14)\n",
    "ax14.imshow(mag_binary_RG_Drk,cmap='gray')\n",
    "ax17 = fig.add_subplot(7,3,17)\n",
    "ax17.imshow(dir_binary_RG,cmap='gray')\n",
    "ax20 = fig.add_subplot(7,3,20)\n",
    "ax20.imshow(combined_RG_Drk,cmap='gray')\n",
    "\n",
    "#plot all images with manipulated color spalce\n",
    "ax3 = fig.add_subplot(733)\n",
    "ax3.set_title('V (HSV) from RG channels', fontsize=20)\n",
    "ax3.imshow(img_RG_V_Drk)\n",
    "ax6 = fig.add_subplot(736)\n",
    "ax6.imshow(gray_RG_V_Drk, cmap='gray')\n",
    "ax9 = fig.add_subplot(739)\n",
    "ax9.imshow(gradx_RG_V_Drk, cmap = 'gray')\n",
    "ax12 = fig.add_subplot(7,3,12)\n",
    "ax12.imshow(grady_RG_V_Drk, cmap = 'gray')\n",
    "ax15 = fig.add_subplot(7,3,15)\n",
    "ax15.imshow(mag_binary_RG_V_Drk,cmap='gray')\n",
    "ax18 = fig.add_subplot(7,3,18)\n",
    "ax18.imshow(dir_binary_RG_V_Drk,cmap='gray')\n",
    "ax21 = fig.add_subplot(7,3,21)\n",
    "ax21.imshow(combined_RG_V_Drk,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(np.mean(cv2.cvtColor(imgDrk_RGB,cv2.COLOR_RGB2GRAY),axis = 0))\n",
    "R = np.array([1117,1110,1079,1095,1101,1089,1012,1096,986,989,1050,1180])\n",
    "L = np.array([326,330,292,273,312,319,322,326,236,240,300,444])\n",
    "print(len(R),len(L), R-L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient conclusions:**\n",
    "* In both the dark and light images, the \"Combined\" image (last image) in the \"HSV\" column (the third column) extract the lane edges the best.\n",
    "\n",
    "In the light image, the horizontal edge of the hood is also detected strongly. However, since that will be a consistent line and since it is not part of the road we can easily filter that out downstream so it doesn't affect the lane calculations.\n",
    "\n",
    "In the final pipeline, we will make sure to only use *Red* and *Green* fields of an image and focus on the *Value* component of the HSV channel using the `extract_lane_gradient` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='calculate-lane-line-curvature'>Calculate Lane Line Curvature</a>\n",
    "Now we transform the edges we detected into a birdseye perspective so we can more accurately measure the curve of the road.\n",
    "___\n",
    "* ** Tangential/Perspective Distortion: **\n",
    "Objects in images look slanted because camera is not aligned perfectly parallel to the imaging plane.\n",
    "(Source Udacity Self Driving Care Notes)\n",
    "$$x_{corrected} = x+[2p_1xy+p_2(r^2+2x^2)]$$\n",
    "$$y_{corrected} = y+[p_1(r^2+2y^2)+2p_2xy]$$\n",
    "\n",
    "Below we transform the road to a birdseye view perspective so we could better understand how the flat road is curving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test warped function:\n",
    "combined_RG_V = extract_lane_gradient(imgLte_RGB, thresh=(20,255), dir_thresh=(0.7,1.3)) \n",
    "\n",
    "offset = 100 # offset for dst points\n",
    "# Grab the image shape\n",
    "img_size = (combined_RG_V.shape[1], combined_RG_V.shape[0])\n",
    "\n",
    "# For source points I'm grabbing the outer four detected corners\n",
    "src = np.float32(\n",
    "    [[(0 + 100), img_size[1]],\n",
    "    [(img_size[0]/2) - 150, (img_size[1]*3 / 5)+75],\n",
    "    [(img_size[0]/2) + 150, (img_size[1]*3 / 5)+75],\n",
    "    [(img_size[0] - 100), (img_size[1])]])\n",
    "    \n",
    "# For destination points, I'm arbitrarily choosing some points to be\n",
    "# a nice fit for displaying our warped result \n",
    "# again, not exact, but close enough for our purposes\n",
    "dst = np.float32(\n",
    "    [[(img_size[0] / 6), img_size[1]],\n",
    "    [(img_size[0] / 6), 0],\n",
    "    [(img_size[0] * 5 / 6), 0],\n",
    "    [(img_size[0] * 5 / 6), img_size[1]]])\n",
    "# Given src and dst points, calculate the perspective transform matrix\n",
    "M = cv2.getPerspectiveTransform(src, dst)\n",
    "# Warp the image using OpenCV warpPerspective()\n",
    "warped = combined_RG_V.copy()\n",
    "warped = cv2.warpPerspective(warped, M, img_size)\n",
    "\n",
    "src_int = np.array(src,np.int32).reshape((-1,1,2))\n",
    "\n",
    "# Draw src polygon lines\n",
    "img = imgLte_RGB.copy()\n",
    "cv2.polylines(img, [src_int],True,color=(255,0,0),thickness=5)\n",
    "\n",
    "# Draw gradient src lines\n",
    "combined_RG_V_copy = combined_RG_V.copy()\n",
    "cv2.polylines(combined_RG_V_copy, [src_int],True,color=(255,0,0),thickness=5)\n",
    "\n",
    "# Draw Birdseye perspective src lines\n",
    "cv2.rectangle(warped,tuple(dst[0]),tuple(dst[2]),(255,0,0), thickness = 5)\n",
    "\n",
    "# Visualize changes\n",
    "fig = plt.figure(figsize=(16,5))\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.set_title('Original Image', fontsize=20)\n",
    "ax1.imshow(img)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.set_title('Binary Gradient', fontsize=20)\n",
    "ax2.imshow(combined_RG_V_copy, cmap = 'gray')\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.set_title('Birdseye Warped', fontsize=20)\n",
    "ax3.imshow(warped, cmap = 'gray')\n",
    "fig.savefig('./output_images/orig_vs_birdseye_warped', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put extract_lane_gradient() and warper() together\n",
    "binary_warped = extract_lane_gradient(imgLte_RGB, thresh=(20,255), dir_thresh=(0.7,1.3)) \n",
    "binary_warped = warper(binary_warped, M)\n",
    "plt.imshow(binary_warped, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#histogram = np.sum(binary_warped[binary_warped.shape[0]//2:,:], axis=0)\n",
    "histogram = np.sum(binary_warped, axis=0)\n",
    "plt.plot(histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Lanes\n",
    "We now use a histogram to find the peaks of where white pixels are to determine where lane lines are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_lane = Line()\n",
    "right_lane = Line()\n",
    "result,left_lane, right_lane = get_lane_pix(binary_warped,left_lane,right_lane)\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize Result\n",
    "# Create an image to draw on and an image to show the selection window\n",
    "#def draw_lanes():\n",
    "out_img = np.dstack((binary_warped, binary_warped, binary_warped))\n",
    "window_img = np.zeros_like(out_img)\n",
    "# Color in left and right line pixels\n",
    "out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "# Generate a polygon to illustrate the search window area\n",
    "# And recast the x and y points into usable format for cv2.fillPoly()\n",
    "left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))])\n",
    "left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, ploty])))])\n",
    "left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))])\n",
    "right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, ploty])))])\n",
    "right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "# Draw the lane onto the warped blank image\n",
    "cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "result = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "plt.imshow(result)\n",
    "plt.plot(left_fitx, ploty, color='yellow')\n",
    "plt.plot(right_fitx, ploty, color='yellow')\n",
    "plt.xlim(0, 1280)\n",
    "plt.ylim(720, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Pipeline on all images in the test_images folder\n",
    "test_image_folder = \"test_images/\"\n",
    "test_image_output_folder = \"output_images/\"\n",
    "\n",
    "video_output_folder = \"output_videos/\"\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "if not os.path.exists(video_output_folder):\n",
    "    os.makedirs(video_output_folder)\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "if not os.path.exists(test_image_output_folder):\n",
    "    os.makedirs(test_image_output_folder)\n",
    "    \n",
    "# Helper method to process image:\n",
    "def process_image(img):\n",
    "    global left_lane\n",
    "    global right_lane\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    # For source points I'm grabbing the outer four detected corners\n",
    "    src = np.float32(\n",
    "    [[(0 + 100), img_size[1]],\n",
    "    [(img_size[0]/2) - 150, (img_size[1]*3 / 5)+75],\n",
    "    [(img_size[0]/2) + 150, (img_size[1]*3 / 5)+75],\n",
    "    [(img_size[0] - 100), (img_size[1])]])\n",
    "\n",
    "    # For destination points, I'm arbitrarily choosing some points to be\n",
    "    # a nice fit for displaying our warped result \n",
    "    # again, not exact, but close enough for our purposes\n",
    "    dst = np.float32(\n",
    "        [[(img_size[0] / 6), img_size[1]],\n",
    "        [(img_size[0] / 6), 0],\n",
    "        [(img_size[0] * 5 / 6), 0],\n",
    "        [(img_size[0] * 5 / 6), img_size[1]]])\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    # First undistort radial distortions\n",
    "    undistort = cal_undistort(img, objpoints, imgpoints)\n",
    "    \n",
    "    # Run through a guassian smoothing\n",
    "    gaussian = cv2.GaussianBlur(undistort,(7,7),0)\n",
    "    \n",
    "    # Extract R & G colors and HSV Value channel\n",
    "    gradient = extract_lane_gradient(gaussian,thresh=(20,255),dir_thresh=(0.7,1.3),ksize = 13,RGB=1)\n",
    "    \n",
    "    # Get lane birdseye perspective\n",
    "    lane = warper(gradient, M)\n",
    "\n",
    "    # Extract lane centers and fill in detected road\n",
    "    detected_lane, left_lane, right_lane = get_lane_pix(lane,left_lane,right_lane)\n",
    "    \n",
    "    # Draw back to original image\n",
    "    unwarped = warper(detected_lane, Minv)\n",
    "\n",
    "    # Cast over the original Image\n",
    "    original = weighted_img(unwarped, img)\n",
    "        \n",
    "    result = write_text(original, left_lane, right_lane)\n",
    "         \n",
    "    # Calculate Lane Curvature\n",
    "    return result\n",
    "\n",
    "#for filename in os.listdir(test_image_folder):\n",
    "    \n",
    "#    img = cv2.imread(os.path.join(test_image_folder,filename))\n",
    "#    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#    output = process_image(img)\n",
    "    \n",
    "    # Uncomment the following code if you are running the code locally and wish to save the image\n",
    "#    mpimg.imsave(os.path.join(test_image_output_folder,\"%s\" %(filename)), output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('frameC53.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "left = Line()\n",
    "right = Line()\n",
    "img_size = [img.shape[1],img.shape[0]]\n",
    "src = np.float32(\n",
    "    [[(0 + 100), img_size[1]],\n",
    "    [(img_size[0]/2) - 150, (img_size[1]*3 / 5)+75],\n",
    "    [(img_size[0]/2) + 150, (img_size[1]*3 / 5)+75],\n",
    "    [(img_size[0] - 100), (img_size[1])]])\n",
    "\n",
    "# For destination points, I'm arbitrarily choosing some points to be\n",
    "# a nice fit for displaying our warped result \n",
    "# again, not exact, but close enough for our purposes\n",
    "dst = np.float32(\n",
    "    [[(img_size[0] / 6), img_size[1]],\n",
    "    [(img_size[0] / 6), 0],\n",
    "    [(img_size[0] * 5 / 6), 0],\n",
    "    [(img_size[0] * 5 / 6), img_size[1]]])\n",
    "\n",
    "M = cv2.getPerspectiveTransform(src, dst)\n",
    "Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "# First undistort radial distortions\n",
    "undistort = cal_undistort(img, objpoints, imgpoints)\n",
    "print(\"undistort.shape\", undistort.shape)\n",
    "\n",
    "# Run through a guassian smoothing\n",
    "gaussian = cv2.GaussianBlur(undistort,(7,7),0)\n",
    "print(\"gaussian.shape\", gaussian.shape)\n",
    "\n",
    "# Extract R & G colors and HSV Value channel\n",
    "gradient = extract_lane_gradient(gaussian,thresh=(20,255),dir_thresh=(0.7,1.3),RGB=1)\n",
    "print(\"gradient.shape\", gradient.shape)\n",
    "\n",
    "# Get lane colored birdseye perspective\n",
    "#lane = gradient.copy()\n",
    "binary_warped = warper(gradient, M)\n",
    "print(\"binary_warped.shape\", binary_warped.shape)\n",
    "\n",
    "result,left, right = get_lane_pix(binary_warped,left,right)\n",
    "#left, right = get_lane_pix(binary_warped,left,right)\n",
    "#print(left.bestx, right.bestx,left.best_fit,right.best_fit,len(right.allx))\n",
    "#print(\"result.shape\", result.shape)\n",
    "\n",
    "\n",
    "# ***********************************************\n",
    "\n",
    "\n",
    "#unwarped = warper(result, Minv)\n",
    "#print(\"unwarped\", unwarped.shape)\n",
    "\n",
    "# Cast over the original Image\n",
    "#original = weighted_img(unwarped, img)\n",
    "#print(\"original\", original.shape)\n",
    "\n",
    "#result = write_text(original,left_lane,right_lane)\n",
    "#print(\"result\", result.shape)\n",
    "#mpimg.imsave(\"test_text.jpg\", result)\n",
    "\n",
    "fig = plt.figure(figsize=(16,5))\n",
    "\n",
    "ax1 = fig.add_subplot(161)\n",
    "ax1.set_title('Original Image', fontsize=20)\n",
    "ax1.imshow(img)\n",
    "ax2 = fig.add_subplot(162)\n",
    "ax2.set_title('undistort', fontsize=20)\n",
    "ax2.imshow(undistort)\n",
    "ax3 = fig.add_subplot(163)\n",
    "ax3.set_title('Gaussian', fontsize=20)\n",
    "ax3.imshow(gaussian)\n",
    "ax4 = fig.add_subplot(164)\n",
    "ax4.set_title('Gradient', fontsize=20)\n",
    "ax4.imshow(gradient)\n",
    "ax5 = fig.add_subplot(165)\n",
    "ax5.set_title('binary_warped', fontsize=20)\n",
    "ax5.imshow(binary_warped)\n",
    "#ax6 = fig.add_subplot(166)\n",
    "#ax6.set_title('result', fontsize=20)\n",
    "#ax6.imshow(original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwindows=9\n",
    "margin = 100\n",
    "minpix =50\n",
    "\"\"\"\n",
    "binary_warped = undistarted and birdseye view warped image\n",
    "left and right = Line() objects\n",
    "nwindows = number of windows desired to split image in y direction when searching for lane pixels\n",
    "margin = x margin from window center to search for additinal lane pixels \n",
    "minpix = min number of pixels needed be considered sufficent to update lane best x-coordinate \n",
    "Returns an image with the lane lines and road highlighted and with radius of curvature and distance from center\n",
    "\"\"\"\n",
    "#print(left.best_fit, right.best_fit)\n",
    "# Create an output image to draw on and  visualize the result\n",
    "out_img = np.dstack((binary_warped, binary_warped, binary_warped)) \n",
    "histogram = np.sum(binary_warped, axis=0)\n",
    "midpoint = np.int(histogram.shape[0]/2)\n",
    "# lane.bestx will be used as the center of the window to detect lane pixels\n",
    "if(left.bestx != None or right.bestx != None): #Use bestx from previous frame\n",
    "    left.detected = True\n",
    "    right.detected = True\n",
    "else: # No previous bestx, use histogram to find peaks\n",
    "    # Take a histogram of the entire birds eye binary image\n",
    "    histogram = np.sum(binary_warped, axis=0)\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    #left.bestx = np.max(np.where(histogram[:midpoint]>20000))\n",
    "    #left.bestx = np.argmax(histogram[:midpoint])\n",
    "    #right.bestx = np.min(np.where(histogram[midpoint:]>20000))+midpoint\n",
    "    #right.bestx = np.argmax(histogram[midpoint:])+midpoint\n",
    "    max = peakdet(histogram, 5000)\n",
    "    split1, split2 =np.split(max,2,axis=1)\n",
    "    clearestLinex = split1[np.argmax(split2)]\n",
    "    if clearestLinex < midpoint: # the left lane\n",
    "        left.bestx = clearestLinex\n",
    "        clearestLineOtherx = left.bestx + 700\n",
    "        x_ind = (split1 > clearestLineOtherx-50) & (split1< clearestLineOtherx+50)\n",
    "        if np.array(x_ind==0).all(): # No other lane detected, return function\n",
    "            #return out_img, left, right\n",
    "            print('pass1')\n",
    "        else:\n",
    "            right.bestx = split1[np.where(split2==np.max(split2[x_ind]))]\n",
    "    else:\n",
    "        right.bestx = clearestLinex\n",
    "        clearestLineOtherx = right.bestx - 700\n",
    "        x_ind = (split1 > clearestLineOtherx-50) & (split1< clearestLineOtherx+50)\n",
    "        if np.array(x_ind==0).all(): # No other lane detected, return function\n",
    "            #return out_img, left, right\n",
    "            print('pass2')\n",
    "        else:\n",
    "            left.bestx = split1[np.where(split2==np.max(split2[x_ind]))]\n",
    "print(left.bestx, right.bestx)\n",
    "plt.plot(histogram)\n",
    "print(np.argmax(histogram[:midpoint]))\n",
    "print(np.argmax(histogram[midpoint:]))\n",
    "print(midpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Run pipeline for videos\n",
    "# Create video Output folder\n",
    "video_output_folder = \"output_videos/\"\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "if not os.path.exists(video_output_folder):\n",
    "    os.makedirs(video_output_folder)\n",
    "project_video = \"%sproject_video.mp4\" % (video_output_folder)\n",
    "\n",
    "left_lane = Line()\n",
    "right_lane = Line()\n",
    "\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "#clip1 = VideoFileClip(\"project_video.mp4\").subclip(0,5)\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "project = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "#%time \n",
    "project.write_videofile(project_video, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_videos/challenge_video.mp4\n",
      "[MoviePy] Writing video output_videos/challenge_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 485/485 [39:44<00:00,  4.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_videos/challenge_video.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "clip1 = VideoFileClip(\"challenge_video.mp4\")\n",
    "#video = clip1.show()(process_image) #NOTE: this function expects color images!!\n",
    "project_video = \"%schallenge_video.mp4\" % (video_output_folder)\n",
    "left_lane = Line()\n",
    "right_lane = Line()\n",
    "video = clip1.fl_image(process_image)\n",
    "\n",
    "#%time \n",
    "video.write_videofile(project_video, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Debugging, save each frame\n",
    "#import cv2\n",
    "vidcap = cv2.VideoCapture(\"harder_challenge_video.mp4\")\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "success = True\n",
    "#while success or count < 10:\n",
    "while count < 10:\n",
    "  success,image = vidcap.read()\n",
    "  #print('Read a new frame: %s' % success)\n",
    "  #cv2.imwrite(\"frameHC%d.jpg\" % count, image)     # save frame as JPEG file\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_videos/harder_challenge_video.mp4\n",
      "[MoviePy] Writing video output_videos/harder_challenge_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ | 125/126 [10:44<00:05,  5.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_videos/harder_challenge_video.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "clip1 = VideoFileClip(\"harder_challenge_video.mp4\")\n",
    "left_lane = Line()\n",
    "right_lane = Line()\n",
    "project_video = \"%sharder_challenge_video.mp4\" % (video_output_folder)\n",
    "white_clip = clip1.fl_image(process_image).subclip(0,5) #NOTE: this function expects color images!!\n",
    "#%time \n",
    "white_clip.write_videofile(project_video, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
